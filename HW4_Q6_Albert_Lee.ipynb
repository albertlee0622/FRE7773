{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4 Q6 RNN & LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "np.random.seed(42)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import SimpleRNN, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the date and split into training/testing sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets before: \n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "Targets after: \n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "\n",
    "print(\"Targets before: \\n{}\".format(y_train[:10]))\n",
    "ybm_train = to_categorical(y_train, n_classes)\n",
    "ybm_test = to_categorical(y_test, n_classes)\n",
    "print(\"Targets after: \\n{}\".format(ybm_train[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "image_size = x_train.shape[1]\n",
    "xrnn_train = np.reshape(x_train,[-1, image_size, image_size])\n",
    "xrnn_test = np.reshape(x_test,[-1, image_size, image_size])\n",
    "xrnn_train = xrnn_train.astype('float32') / 255\n",
    "xrnn_test = xrnn_test.astype('float32') / 255\n",
    "print(xrnn_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 : Explain why we reshape data as [-1, image_size, image_size]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  \n",
    "RNN and LSTM require input data to have $[Number~of~Samples,~Time~Steps,~Features]$  \n",
    "In this case, one training image is devided into $28~x~28$, and we are sending each row with 28 features sequentially.   \n",
    "For $[-1,~image\\_size,~image\\_size]$, -1 can be used to define unknown values, so in this this case, it will automatically assign 60000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : Write code to set parameters for RNN. \n",
    "#### 1. You need to set input shape, layers (2), units (256), dropout_rate (0.4), activation function ('relu'）\n",
    "#### 2. You may use Sequential, SimpleRNN, Dropout\n",
    "#### 3. You need to add one dense layer at the end of your network. (You may use : Dense, activation function is 'softmax')\n",
    "#### 4. You need to summary the parameters (You may use summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 28, 256)           72960     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 206,858\n",
      "Trainable params: 206,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(SimpleRNN(units=256, input_shape=(image_size, image_size), return_sequences=True))\n",
    "model1.add(Dropout(rate=0.4))\n",
    "\n",
    "model1.add(SimpleRNN(units=256, input_shape=(image_size, image_size)))\n",
    "model1.add(Dropout(rate=0.4))\n",
    "\n",
    "model1.add(Dense(units=n_classes, activation='softmax'))\n",
    "\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 : Write code to implement RNN.\n",
    "#### 1. Compile the model (you may use compile, optimizer as 'nadam', loss as 'categorical_crossentropy', metrics as ['accuracy'])\n",
    "#### 2. Set early stop, monitor as 'val_loss', patience as 3, mode as 'auto', min_delta as 0.  (you may use EarlyStopping)\n",
    "#### 3. Fit x_data and remember to set callback (Set batch_size as 1000, epochs as 10, validation_split as 0.1)\n",
    "#### 4. Print out the accuracy of train set and test set of each epoch (you may use evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alee8\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 25s 456us/step - loss: 0.9723 - acc: 0.6863 - val_loss: 0.3141 - val_acc: 0.9035\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 23s 434us/step - loss: 1.8064 - acc: 0.4650 - val_loss: 0.5393 - val_acc: 0.8442\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 24s 439us/step - loss: 0.8246 - acc: 0.7507 - val_loss: 2.4229 - val_acc: 0.3795\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 27s 496us/step - loss: 0.7563 - acc: 0.7674 - val_loss: 0.2355 - val_acc: 0.9283\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 26s 472us/step - loss: 0.3479 - acc: 0.8971 - val_loss: 0.1536 - val_acc: 0.9553\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 26s 486us/step - loss: 0.2847 - acc: 0.9184 - val_loss: 0.1462 - val_acc: 0.9578\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 25s 454us/step - loss: 0.1805 - acc: 0.9490 - val_loss: 0.1064 - val_acc: 0.9685\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 27s 497us/step - loss: 0.1479 - acc: 0.9576 - val_loss: 0.0990 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 27s 498us/step - loss: 0.2771 - acc: 0.9338 - val_loss: 4.6130 - val_acc: 0.2492\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 26s 489us/step - loss: 2.1737 - acc: 0.2796 - val_loss: 1.0064 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='auto', min_delta=0, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history1 = model1.fit(xrnn_train, ybm_train, batch_size=1000, epochs=10, validation_split=0.1, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history1.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss:  [0.9722806132502027, 1.8063754152368616, 0.8246483482696392, 0.756297657335246, 0.3478739556890947, 0.28468208070154544, 0.18048053897089428, 0.14785883451501527, 0.27709640755697534, 2.173657907379998]\n",
      "Train_accuracy:  [0.6862962960645005, 0.46503703944661, 0.7506851851940155, 0.7674074084670456, 0.8971481466734851, 0.9183888854803862, 0.9489999998498846, 0.9575555589463975, 0.9337962984486863, 0.2796111122049667]\n",
      "Validation_loss:  [0.3141018922130267, 0.5393432229757309, 2.4229359229405723, 0.2355282505353292, 0.15360453724861145, 0.1461922898888588, 0.10643813262383144, 0.09897683560848236, 4.612960497538249, 1.0063961446285248]\n",
      "Validation_accuracy:  [0.9034999907016754, 0.8441666762034098, 0.37950000166893005, 0.9283333222071329, 0.9553333322207133, 0.957833339770635, 0.9684999883174896, 0.971833328406016, 0.24916666001081467, 0.6666666666666666]\n"
     ]
    }
   ],
   "source": [
    "print('Train_loss: ', history1.history['loss'])\n",
    "print('Train_accuracy: ', history1.history['acc'])\n",
    "print('Validation_loss: ', history1.history['val_loss'])\n",
    "print('Validation_accuracy: ', history1.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 394us/step\n"
     ]
    }
   ],
   "source": [
    "test_result1 = model1.evaluate(xrnn_test, ybm_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss : 1.1295365725517272\n",
      "Test_accuracy : 0.6157\n"
     ]
    }
   ],
   "source": [
    "print('Test_loss :', test_result1[0])\n",
    "print('Test_accuracy :', test_result1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 : Write code to set parameters for LSTM\n",
    "#### 1. You need to set input shape, layers (2), units (256), dropout_rate (0.4), activation function ('relu'）\n",
    "#### 2. You may use Sequential, LSTM, Dropout\n",
    "#### 3. You need to add one dense layer at the end of your network. (You may use : Dense, activation function is 'softmax')\n",
    "#### 4. You need to summary the parameters (You may use summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 28, 256)           291840    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 28, 256)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 425,738\n",
      "Trainable params: 425,738\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(LSTM(units=256, input_shape=(image_size, image_size), return_sequences=True))\n",
    "model2.add(Dropout(rate=0.4))\n",
    "\n",
    "model2.add(SimpleRNN(units=256, input_shape=(image_size, image_size)))\n",
    "model2.add(Dropout(rate=0.4))\n",
    "\n",
    "model2.add(Dense(units=n_classes, activation='softmax'))\n",
    "\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 : Write code to implement LSTM.\n",
    "#### 1. Compile the model (you may use compile, optimizer as 'nadam', loss as 'categorical_crossentropy', metrics as ['accuracy'])\n",
    "#### 2. Set early stop, monitor as 'val_loss', patience as 3, mode as 'auto', min_delta as 0.  (you may use EarlyStopping)\n",
    "#### 3. Fit x_data and remember to set callback (Set batch_size as 1000, epochs as 10, validation_split as 0.1)\n",
    "#### 4. Print out the accuracy of train set and test set of each epoch (you may use evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 67s 1ms/step - loss: 1.1075 - acc: 0.6272 - val_loss: 0.2830 - val_acc: 0.9132\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 55s 1ms/step - loss: 0.3212 - acc: 0.8989 - val_loss: 0.1335 - val_acc: 0.9587\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 54s 994us/step - loss: 0.5178 - acc: 0.8712 - val_loss: 3.4715 - val_acc: 0.2185\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 53s 989us/step - loss: 0.9429 - acc: 0.7149 - val_loss: 0.1686 - val_acc: 0.9457\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 52s 962us/step - loss: 0.1922 - acc: 0.9433 - val_loss: 0.1129 - val_acc: 0.9643\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 52s 957us/step - loss: 0.1198 - acc: 0.9650 - val_loss: 0.0731 - val_acc: 0.9783\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 53s 979us/step - loss: 0.0957 - acc: 0.9724 - val_loss: 0.0678 - val_acc: 0.9805\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 52s 966us/step - loss: 0.0715 - acc: 0.9795 - val_loss: 0.0598 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 52s 959us/step - loss: 0.0603 - acc: 0.9822 - val_loss: 0.0889 - val_acc: 0.9750\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 52s 960us/step - loss: 0.0553 - acc: 0.9841 - val_loss: 0.0514 - val_acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', patience=3, mode='auto', min_delta=0, verbose=1, restore_best_weights=True)\n",
    "\n",
    "history2 = model2.fit(xrnn_train, ybm_train, batch_size=1000, epochs=10, validation_split=0.1, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_loss:  [1.107492118521973, 0.32123406921271924, 0.5178311298842784, 0.942856385200112, 0.19224478194007166, 0.11981843246353997, 0.09565970798333485, 0.07147593589292632, 0.060326035858856306, 0.055254240727259055]\n",
      "Train_accuracy:  [0.6272037029266357, 0.898888885974884, 0.8712037031849226, 0.7148518531962678, 0.9432962912100332, 0.9650370368251094, 0.9724444459985804, 0.979499997916045, 0.9822222215157969, 0.9841296286494644]\n",
      "Validation_loss:  [0.28299641609191895, 0.1335458941757679, 3.4714513222376504, 0.16863589733839035, 0.11287385101119678, 0.07306283339858055, 0.06777001669009526, 0.05983287406464418, 0.08887973117331664, 0.051382586980859436]\n",
      "Validation_accuracy:  [0.9131666620572408, 0.9586666623751322, 0.21849999825159708, 0.9456666807333628, 0.9643333355585734, 0.9783333440621694, 0.9805000027020773, 0.9830000102519989, 0.975000003973643, 0.9848333299160004]\n"
     ]
    }
   ],
   "source": [
    "print('Train_loss: ', history2.history['loss'])\n",
    "print('Train_accuracy: ', history2.history['acc'])\n",
    "print('Validation_loss: ', history2.history['val_loss'])\n",
    "print('Validation_accuracy: ', history2.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 725us/step\n"
     ]
    }
   ],
   "source": [
    "test_result2 = model2.evaluate(xrnn_test, ybm_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss : 0.05856194389555603\n",
      "Test_accuracy : 0.9818\n"
     ]
    }
   ],
   "source": [
    "print('Test_loss :', test_result2[0])\n",
    "print('Test_accuracy :', test_result2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
